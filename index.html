<!DOCTYPE HTML>
<html>
	<head>
		<title>Colorization</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body id="top">

		<!-- Header -->
		<header id="header" class="skel-layers-fixed">
			<h1><a href="#">TEAM 9</a></h1>
			<nav id="nav">
				<ul>
					<li><a href="#authors">Authors</a></li>
					<li><a href="#introduction">Introduction</a></li>
 					<li><a href="#dataset">Dataset generation</a></li>
					<li><a href="#architecture">Architecture</a></li>
					<li><a href="#experiments">Experiments</a></li>
					<li><a href="#problems_solutions">Problems and Solutions</a></li>
					<li><a href="#Results">Results</a></li>
					<li><a href="#future_work">Future Work</a></li>
					
				</ul>
			</nav>
		</header>

		<!-- Banner -->
		<section id="banner">
			<div class="inner">
				<h2></h2>
			</div>
		</section>

		<!-- One -->
		<section id="authors" class="wrapper style1">
			<header class="major">
					<h2>Team members</h2>
			</header>

			<div class="container">
				<div class="row">
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/adribarja/" class="image fit"><img src="images/adria.jpg" alt="" /></a>
							<p>Adrià Barja</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/clara-bonn%C3%ADn-rossell%C3%B3-18634b10b/" class="image fit"><img src="images/clara.jpg" alt="" /></a>
							<p>Clara Bonnín</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/jmarcorimmek/" class="image fit"><img src="images/joan.jpg" alt="" /></a>
							<p>Joan Marco Rimmek</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/mariona-c-a7bb91105/" class="image fit"><img src="images/mariona.png" alt="" /></a>
							<p>Mariona Carós</p>
						</section>
					</div>
				</div>
			</div>
		</section>

		<!-- Three -->
		<section id="introduction" class="wrapper style1">
			<header class="major">
				<h2>Introduction</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Abstract</h2>
							<a class="image fit"><img src="images/colors.jpeg" alt="" /></a>
							<p> The aim of this project is to generate realistic colorized images given a grayscale photograph as input , following and reproducing the ideas proposed in <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization by Richard Zhang, Phillip Isola and Alexei A. Efros </a>.<br><br>

							This paper formulates image colorization as a pixel level multimodal classification problem and Deep Learning techniques are used to solve it. In particular, the model consists in a Convolutional Neural Network (CNN) with multiple layers that we implemented from scratch with Keras and Tensorflow (The framework used in the paper is Caffe). Given the source luminance, taken from the grayscale image, the model predicts the corresponding a and b color channels of the image in the CIE Lab colorspace.
							</p>
							<a class="image fit"><img src="images/approach.png" alt="" /></a>
						</section>
					</div>
					<div class="6u">
						<section>
							<h3>State of the art</h3>
							<p>Similar systems have been develoed by other works, as <a href= "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_35"> Larsson et al.</a> , <a href= " https://dl.acm.org/citation.cfm?id=2925974"> Iizuka et al.</a>   <a href= "https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Cheng_Deep_Colorization_ICCV_2015_paper.pdf" >Cheng Z et al.</a> ), however the results tend to be desaturated. 

							The methods differ in their CNN architectures and loss functions. While our paper  uses a weighted classiffication loss to rebalance rare classes, Larsson et al. use an un-rebalanced classiffcation loss, and Iizuka et al. use a regression loss. About the CNN architectures. Larsson et al. use hypercolumns on a VGG network and Iizuka et al. use a two-stream architecture in which they use global and local features.

							Generative Adversarial Nets (GAN) have also been used to predict color (i. e.  <a href= "http://cs231n.stanford.edu/reports/2017/pdfs/302.pdf" > Qiwen Fu et al.</a> ).
							</p>


						</section>	
						<section>
							<h3>Dataset</h3>
							<a class="image fit"><img src="images/BW_bird.png" alt="" /></a>
							<p> Two  datasets were used to train our model; the well known <a href="http://www.image-net.org"> ImageNet </a> and a flowers image dataset composed of several <a href= "https://www.kaggle.com/alxmamaev/flowers-recognition"> datasets from Kaggle</a> containing 14K images. Predicting color has the nice property that training data is practically free. There is no need for humans to label data because any color photo can be used as a training example, simply by taking the image's L channel (luminance) as input and its ab channels (color) as the supervisory signal. Therefore, we can say our algorithm uses a self-supervised approach to train by previously converting the source images to grayscale.<br><br>

							We decided to use the flowers dataset because the model was training very slow with ImageNet, as it contains a large amount of images, and we were running out of time. </p>
						</section>
					</div>
				</div>
			</div>
		</section>			

		<section id="dataset" class="wrapper style1">
			<header class="major">
				<h2>Dataset generation</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="7u">
						<section>
							<h2>CIE Lab Color space</h2>
							<a class="image fit"><img src="images/Quantized_ab_color_space.png" alt=""/></a>
							<p> We work on the CIE Lab Color space. It expresses color as three numerical values, L* for the lightness and a* and b* for the green–red and blue–yellow color components. CIELAB was designed to be perceptually uniform with respect to human color vision, meaning that the same amount of numerical change in these values corresponds to about the same amount of visually perceived change.</p>
						</section>
					</div>
					<div class="5u">
						<section>
							<h2>Class rebalancing</h2>
							<p> Explain class rebalancing and class Probabilities to Point estimates</p>
						</section>
					</div>
				</div>
			</div>
		</section>

		<section id="architecture" class="wrapper style1">
			<header class="major">
				<h2>Architecture</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="7u">
						<section>
							<h2>Approach</h2>
							<a class="image fit"><img src="images/architecture.png" alt="" /></a>
							<p> The architecture proposed in the <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization paper </a> consists in a CNN composed by 8 layers.
							Each layer refers to a block of 2 or 3 repeated convolution layers with a 3x3 filter and a ReLU activation function, followed by a Batch Normalization layer. To preserve the resolution within the block, a padding filter is used. <br> <br>

							At conv5 and conv6 a dilated convolution is applied to keep the output resolutions high and avoid the need of upsampling. A dilated convolution is a convolution with gaps (filled in with '0') in the filter. It is effective for a broader view of the input to capture more contextual information and it enables a faster run-time with less parameters. <br> <br>

							The following table lists the layers used in the architecture during training time.

							<a class="image fit"><img src="images/table_cnn.png" alt="" /></a>

							<b>X</b>: Spatial resolution of output <br> 
							<b>C</b>: Number of channels of output <br>
							<b>S</b>: Computation stride, values greater than 1 indicate downsampling following convolution, values less than 1 indicate upsampling preceding convolution <br>
							<b>D</b>: Kernel dilation <br>
							<b>Sa</b>: Accumulated stride across all preceding layers <br>
							<b>De</b>: Effective dilation of the layer with respect to the input (layer dilation times accumulated stride)<br>
							<b>BN</b>: Whether BatchNorm layer was used after layer
							<b>L</b>: Whether a 1x1 conv and cross-entropy loss layer was imposed.<br> <br>

							The model does not have any pool layer, all changes in size between conv blocks are achieved through spatial downsampling (by increasing stride) or upsampling.
							</p>
						</section>
					</div>
					<div class="5u">
						<section>
							<h2>Loss Function: Weighted cross-entropy</h2>
							<a class="image fit"><img src="images/loss_function.png" alt="" /></a>
							<p>The problem is treated as a multinomial classification. The ab output is quantified in Q=313 values. For a given input X of dims HxWx1, our architecture learns a mapping Zpred = G(X) to a probability distribution of the Q possible colors values. Therefore, Zpred has dimensions HxWxQ. <br>
							In order to compare Z predicted against the ground truth values of the image Y, we define a soft-encoding Z = inverse(H)(Y) that transforms the ground truth color to a 1-hot vector Z with the nearest 5 quantized bins ab using a gaussian kernel.<br>
							To account for the imbalance problem, a reweighting of the loss function based in the pixel's rarity is defined. Each pixels is weighted by a factor w of dims Qx1 based on its closest ab bin. The following formula shows how the reweighting matrix is obtained. </p>
						</section>
					</div>
				</div>

			</div>
		</section>

		<section id="experiments" class="wrapper style1">
			<header class="major">
				<h2>Experiments</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Overfitting a mini-batch</h2>
							</a>
							<p> The first resuts we obtained from our model evidenced that it was not able to predict correctly any color of the input image and the predictions seemed to be random.
								<br><br>
							<a class="image fit"><img src="images/first_results.png" alt="" /></a>
							Thus, we decided to overfit the model by using a mini-batch of 25 images to ensure that the model was working properly. This technique is a way of doing a quick sanity test because if the model cannot overfit a small amount of data, there is a simple bug somewhere. Once we achieved to overfit the model, we trained the model with more inputs.
							<br><br>
							<a class="image fit"><img src="images/mini_batch_overfitted.png" alt=""/></a>
							</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Trainig with different loss functions</h2>
							<p> We decided to train our model with two different loss functions: The Mean Squared Error and the Weighted Cross Entropy. We obtained two different models to test and we compared the results. We prove that the Weighted Cross Entropy is the loss that provides the more bright and realistic results. </p>
							<img src="images/mse_mini_batch.png" alt="" style="width:512px;"> 
							
							
						</section>
					</div>
				</div>	
			</div>					
		</section>

		<section id="problems_solutions" class="wrapper style1">
			<header class="major">
				<h2>Problems and solutions</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="4u">
						<section>
							<h2>Loss turns into 'nan'</h2>
							<p> Explain</p>
						</section>
					</div>
					<div class="4u">
						<section>
							<h2>The model only produces red images</h2>
							<a class="image fit"><img src="images/problem_red_outputs.png" alt="" /></a>
							<p> Explain </p>
						</section>
					</div>
					<div class="4u">
						<section>
							<h2>First results</h2>
							<a class="image fit"><img src="images/first_results.png" alt="" /></a>
							<p> Explain </p>
						</section>
					</div>
				</div>	
			</div>
		</section>
		
		<section id="Results" class="wrapper style1">
			<header class="major">
				<h2>Results</h2>
			</header>
			<div class="container">
				
			</div>
		</section>
		
		<section id="future_work" class="wrapper style1">
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Future work</h2>
							<p>....</p>
						</section>
					</div>

				</div>
			</div>
		</section>
		

	</body>
</html>
