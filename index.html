<!DOCTYPE HTML>
<html>
	<head>
		<title>Colorization</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body id="top">

		<!-- Header -->
		<header id="header" class="skel-layers-fixed">
			<h1><a href="#">TEAM 9</a></h1>
			<nav id="nav">
				<ul>
					<li><a href="#authors">Authors</a></li>
					<li><a href="#introduction">Introduction</a></li>
 					<li><a href="#dataset">Dataset generation</a></li>
					<li><a href="#architecture">Architecture</a></li>
					<li><a href="#experiments">Experiments</a></li>
					<li><a href="#problems_solutions">Problems and Solutions</a></li>
					<li><a href="#Results">Results</a></li>
					<li><a href="#future_work">Future Work</a></li>
					
				</ul>
			</nav>
		</header>

		<!-- Banner -->
		<section id="banner">
			<div class="inner">
				<h2></h2>
			</div>
		</section>

		<!-- One -->
		<section id="authors" class="wrapper style1">
			<header class="major">
					<h2>Team members</h2>
			</header>

			<div class="container">
				<div class="row">
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/adribarja/" class="image fit"><img src="images/adria.jpg" alt="" /></a>
							<p>Adrià Barja</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/clara-bonn%C3%ADn-rossell%C3%B3-18634b10b/" class="image fit"><img src="images/clara.jpg" alt="" /></a>
							<p>Clara Bonnín</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/jmarcorimmek/" class="image fit"><img src="images/joan.jpg" alt="" /></a>
							<p>Joan Marco Rimmek</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/mariona-c-a7bb91105/" class="image fit"><img src="images/mariona.png" alt="" /></a>
							<p>Mariona Carós</p>
						</section>
					</div>
				</div>
			</div>
		</section>

		<!-- Three -->
		<section id="introduction" class="wrapper style1">
			<header class="major">
				<h2>Introduction</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Our approach</h2>
							<a class="image fit"><img src="images/colors.jpeg" alt="" /></a>
							<p> We proposed ourselves a risky problem: start from scratch a neural network in order to colorize grayscale images following the ideas proposed in <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization by Richard Zhang, Phillip Isola and Alexei A. Efros </a>.<br><br>

							This paper formulates image colorization as a pixel level multimodal classification problem and Deep Learning techniques are used to solve it. Firstly, we had to look for a dataset. As an initial approach, we tried with Imagenet. Then, we build a Convolutional Neural Network with multiple layers using the sequential model of Keras and after that, we analysed the performance of two objective functions.<br><br>

							As we can see on the image, the model predicts automatically, without using any image enhancement,  the corresponding a and b color channels of the image in the CIE Lab colorspace given the source luminance, taken from the grayscale image.

							</p>
							<a class="image fit"><img src="images/approach.png" alt="" /></a>
						</section>
					</div>
					<div class="6u">
						<section>
							<h3>State of the art</h3>
							<p>Similar systems have been develoed by other works, as <a href= "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_35"> Larsson et al.</a> , <a href= " https://dl.acm.org/citation.cfm?id=2925974"> Iizuka et al.</a>   <a href= "https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Cheng_Deep_Colorization_ICCV_2015_paper.pdf" >Cheng Z et al.</a> ), however the results tend to be desaturated. 

							The methods differ in their CNN architectures and loss functions. While we use a weighted classification loss to rebalance rare classes, Larsson et al. use an un-rebalanced classification loss in a VGG network and Iizuka et al. use a regression loss in a two-stream architecture. 

							Generative Adversarial Nets (GAN) have also been used to predict color (i. e.  <a href= "http://cs231n.stanford.edu/reports/2017/pdfs/302.pdf" > Qiwen Fu et al.</a> ).
							</p>


						</section>	
						<section>
							<h3>Dataset</h3>
							<a class="image fit"><img src="images/BW_bird.png" alt="" /></a>
							<p> Two  datasets were used to train our model; the well known <a href="http://www.image-net.org"> ImageNet </a> and a flowers image dataset composed of several <a href= "https://www.kaggle.com/alxmamaev/flowers-recognition"> datasets from Kaggle</a> containing 14K images. Predicting color has the nice property that training data is practically free. There is no need for humans to label data because any color photo can be used as a training example, simply by taking the image's L channel (luminance) as input and its ab channels (color) as the supervisory signal. Therefore, we can say our algorithm uses a self-supervised approach to train by previously converting the source images to grayscale.<br><br>

							We decided to use the flowers dataset because the model was training very slow with ImageNet, as it contains a large amount of images, and we were running out of time. </p>
						</section>
					</div>
				</div>
			</div>
		</section>			

		<section id="dataset" class="wrapper style1">
			<header class="major">
				<h2>Dataset generation</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="7u">
						<section>
							<h2>CIE Lab Color space</h2>
							<a class="image fit"><img src="images/Quantized_ab_color_space.png" alt=""/></a>
							<p> We work on the CIE Lab Color space. It expresses color as three numerical values, L* for the lightness and a* and b* for the green–red and blue–yellow color components. CIELAB was designed to be perceptually uniform with respect to human color vision, meaning that the same amount of numerical change in these values corresponds to about the same amount of visually perceived change.</p>
						</section>
					</div>
					<div class="5u">
						<section>
							<h2>Class rebalancing</h2>
							<p> As explained in the paper our work is based on, not all colors have the same probability to appear in an image. In particular, grayish colors are the most probable. Therefore a neural net predicting only grayish colors has the highest chance of making a correct prediction. In order to avoid this behavior, a weighted categorical cross entropy loss is defined (explained later) which penalizes grayish predictions in order to encourage the net to make more vivid and saturated predictions. The weights used in this custom loss depend on the probability of each color to appear. Since defining a continuous probability function for each color would be hardly feasible, their approach consists of discretizing the color space in 313 different ab values. For each of those 313 values, their probability is computed using the ImageNet database. <br>

							The discretization of 313 values can be found in the pts_in_hull.npy file and their probability in prior_probs.npy, both under the data/ directory. We copied these arrays from the repository of the original paper since computing them required processing the whole ImageNet dataset and was hardly teaching us much about deep learning. </p>
						</section>
					</div>
				</div>
			</div>
		</section>

		<section id="architecture" class="wrapper style1">
			<header class="major">
				<h2>Architecture</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="7u">
						<section>
							<h2>Approach</h2>
							<a class="image fit"><img src="images/architecture.png" alt="" /></a>
							<p> The architecture proposed in the <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization paper </a> consists in a CNN composed by 8 layers.
							Each layer refers to a block of 2 or 3 repeated convolution layers with a 3x3 filter and a ReLU activation function, followed by a Batch Normalization layer. To preserve the resolution within the block, a padding filter is used. <br> <br>

							At conv5 and conv6 a dilated convolution is applied to keep the output resolutions high and avoid the need of upsampling. A dilated convolution is a convolution with gaps (filled in with '0') in the filter. It is effective for a broader view of the input to capture more contextual information and it enables a faster run-time with less parameters. <br> <br>

							The following table lists the layers used in the architecture during training time.

							<a class="image fit"><img src="images/table_cnn.png" alt="" /></a>

							<b>X</b>: Spatial resolution of output <br> 
							<b>C</b>: Number of channels of output <br>
							<b>S</b>: Computation stride, values greater than 1 indicate downsampling following convolution, values less than 1 indicate upsampling preceding convolution <br>
							<b>D</b>: Kernel dilation <br>
							<b>Sa</b>: Accumulated stride across all preceding layers <br>
							<b>De</b>: Effective dilation of the layer with respect to the input (layer dilation times accumulated stride)<br>
							<b>BN</b>: Whether BatchNorm layer was used after layer
							<b>L</b>: Whether a 1x1 conv and cross-entropy loss layer was imposed.<br> <br>

							The model does not have any pool layer, all changes in size between conv blocks are achieved through spatial downsampling (by increasing stride) or upsampling.
							</p>
						</section>
					</div>
					<div class="5u">
						<section>
							<h2>Loss Functions: MSE and weighted cross-entropy</h2>
							<p>We have tested the two objective functions proposed in :  <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization</a> MSE and a weighted multinomial cross-entropy. The input of our baseline CNN is the luminance channel X of an image with the size (H,W,1) and the output Z_pred is the predicted probability distribution over a set of possible colors of shape (H,W,313), as it is explained in the class rebalancing section.
							<br>
							In order to compare Z_pred predicted against the ground truth, we need to define a Z which it is the result of a soft-encoding from the truth colors to the space of quantized values. This Z is a soft 1-hot vector for the 5-nearest neighbors weighted with a Gaussinan kernel according to their distance to the truth color.
							The first proposed objective function is the Mean-Squared Error loss between the ground truth Z and the predicted values Z_pred. This loss function was already implemented by Keras.<br>

							The MSE is not robust to inherent ambiguity and multimodal nature of the colorization problem. The MSE results in giving the mean color of the set to all images, in other words, giving grayish and desaturated images.
							<br>
							To counter this effect, it is proposed a second approach: the weighted categorical cross-entropy.  The reweighting of the objective function is done to account for the imbalance problem, as not all colors have the same frequency of appearance.<br>

							<a class="image fit"><img src="images/loss_function.png" alt="" /></a>

							The weights are defined according the prior smoothed empirical distribution obtained from the repository of the original paper. This custom loss function was implemented using Tensorflow and can be found in the loss.py script.  </p>
						</section>
					</div>
				</div>

			</div>
		</section>

		<section id="experiments" class="wrapper style1">
			<header class="major">
				<h2>Experiments</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Overfitting a mini-batch</h2>
							</a>
							<p> Once set up correctly, we got our first results. After 30 minutes we got an intuition that we were doing well!
								<br><br>
							<a class="image fit"><img src="images/first_results.png" alt="" /></a>
							We decided to overfit the model for some hours by using a mini-batch of 25 images to ensure that the model was working properly. This technique is a way of doing a quick sanity check because if the model cannot overfit a small amount of data, there is a simple bug somewhere! 
							<br><br>
							<a class="image fit"><img src="images/mini_batch_overfitted.png" alt=""/></a>
							Once we achieved to overfit the model, we started to train the model with the whole dataset. 

							</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Trainig with different loss functions</h2>
							<p> We decided to train our model with two different loss functions to compare the results. We used the Mean Squared Error and the Weighted Cross Entropy.
							The results obtained can be found in the last section, we can easily see that the model with the Weighted Cross Entropy performs much better than the Mean Squared Error does. The predicted images with the Weighted Cross Entropy are more similar to the ground truth and more saturated and realistic </p>
							<img src="images/mse_mini_batch.png" alt="" style="width:512px;"> 
							
							
						</section>
					</div>
				</div>	
				<div class="row">
					<div class="6u">
						<section>
							<h2>Trainig with different architectures: UNet</h2>
							<p> Image colorization is an image-to-image problem and can be understood as pixel-wise regression problem. In order to analyse our problem from another approach we tried to understand it as it: Our input greyscale image could be classified in Q possible classes, in other words, in the Q possible quantized colors values. A known and not difficult to implement architecture was the U-Net. So we implemented a 2D U-Net using Keras framework and tried to do overfit to one mini-batch. However, we discovered that the network we had designed had not capacity enough to deal with our problem as it was unable to overfit un mini-batch of size 15 for 20k iterations. Our 2D U-Net had 712.177 parameters against the 32.240.377 of the baseline architecture. A future experiment could be increasing the number of filters for each convolutional layer. The following figures are the training loss function and a predicted sample of the mini-batch. </p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Trainig with different architectures: Conditional GAN</h2>
							<p> explain. </p>
						</section>
					</div>
				</div>	
			</div>					
		</section>

		<section id="problems_solutions" class="wrapper style1">
			<header class="major">
				<h2>Problems and solutions</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Loss turns into 'nan'</h2>
							<p> In the model we were trying to replicate, the output were the logits and the loss functions included the computation of the softmax layer before comparing. For this reason we did not realize that we were missing in our model a final softmax somewhere between the output of the model and the loss function. <br>
							Since there was no activation. When training with our self-programmed loss function, we were computing logarithms of negative numbers thus resulting in NaNs. Somehow, the default cross entropy loss, was robust to that kind of input and did not fail as our loss, which lead us to think that what was wrong was the loss and not the output of the net.
							</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>The model only produces red images</h2>
							<a class="image fit"><img src="images/problem_red_outputs.png" alt="" /></a>
							<p> Related to the previous problem, we tried to constrain the output of the model with relus and sigmoids and add a personalized softmax inside the loss function. Therefore the net was allowed to predict 1 in all the 64*64*313 output values. The network was doing this because, since we were using a cross entropy loss which performs the logarithm of the output values and since the logarithm of one is zero, outputting all ones was minimizing the loss. Then, in the prediction phase, the color is given by the output times the corresponding 313 colors. Since all the outputs where ones for each pixel, the expectation yielded the mean value, which for the CIE Lab color space seem to be this pink-ish slash red-ish color.<br>
							With these last two problems we can conclude that, although in principle a neural net with enough capacity can learn anything, if the output domain is similar to the label domain, we will have a much better perfomance and obtain better and quicker results.
 							</p>
						</section>
					</div>
				</div>	
				<div class="row">
					<div class="6u">
						<section>
							<h2>Imagenet is too large</h2>
							<p> The main reason we experimented with two different datasets is simple. Once everything was set up, we saw that we had not enough time to train with Imagenet. Each epoch lasted 130h, so we decided to find another smaller dataset with a well defined input domain (not as imagenet which has a lot of different categories). <br>
							We choosed a mix of flower datasets which would simplify the task and would not require so many training samples. With this approach we reduced the training time and quickly improved the results!

							</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Keras changes our loss</h2>
							<p> When we reloaded our saved model, Keras changed our customized weighted crossentropy since name was the same. For these reason, one of our experiments consists in an hybrid-loss model which is trained with two different losses. We saw that the normal crossentropy is not a bad choice of loss function although, as we have seen, it does not take into account the prior color distribtuion.
 							</p>
						</section>
					</div>

				</div>
			</div>
		</section>
		
		<section id="Results" class="wrapper style1">
			<header class="major">
				<h2>Results</h2>
			</header>
			<div class="container">
				
			</div>
		</section>
		
		<section id="future_work" class="wrapper style1">
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Future work</h2>
							<p>....</p>
						</section>
					</div>

				</div>
			</div>
		</section>
		

	</body>
</html>
