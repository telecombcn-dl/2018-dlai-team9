<!DOCTYPE HTML>
<html>
	<head>
		<title>Colorization</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="js/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-layers.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-xlarge.css" />
		</noscript>
	</head>
	<body id="top">

		<!-- Header -->
		<header id="header" class="skel-layers-fixed">
			<h1><a href="#">TEAM 9</a></h1>
			<nav id="nav">
				<ul>
					<li><a href="#authors">Authors</a></li>
					<li><a href="#introduction">Introduction</a></li>
 					<li><a href="#dataset">Dataset generation</a></li>
					<li><a href="#architecture">Architecture</a></li>
					<li><a href="#experiments">Experiments</a></li>
					<li><a href="#problems_solutions">Problems and Solutions</a></li>
					<li><a href="#Final approach: Colorization">Final approach</a></li>
					<li><a href="#future_work">Future Work</a></li>
					
				</ul>
			</nav>
		</header>

		<!-- Banner -->
		<section id="banner">
			<div class="inner">
				<h2></h2>
			</div>
		</section>

		<!-- One -->
		<section id="authors" class="wrapper style1">
			<header class="major">
					<h2>Team members</h2>
			</header>

			<div class="container">
				<div class="row">
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/adribarja/" class="image fit"><img src="images/adria.jpg" alt="" /></a>
							<p>Adrià Barja</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/clara-bonn%C3%ADn-rossell%C3%B3-18634b10b/" class="image fit"><img src="images/clara.jpg" alt="" /></a>
							<p>Clara Bonnín</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/jmarcorimmek/" class="image fit"><img src="images/joan.jpg" alt="" /></a>
							<p>Joan Marco Rimmek</p>
						</section>
					</div>
					<div class="3u">
						<section class="special box">
							<a href="https://www.linkedin.com/in/mariona-c-a7bb91105/" class="image fit"><img src="images/mariona.png" alt="" /></a>
							<p>Mariona Carós</p>
						</section>
					</div>
				</div>
			</div>
		</section>

		<!-- Three -->
		<section id="introduction" class="wrapper style1">
			<header class="major">
				<h2>Introduction</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Abstract</h2>
							<a class="image fit"><img src="images/colors.jpeg" alt="" /></a>
							<p> The aim of this project is to generate realistic colorized images given a grayscale photograph as input, following and reproducing the ideas proposed in <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization by Richard Zhang, Phillip Isola and Alexei A. Efros </a> that was implemented in Caffe.<br><br>

							The model consists in a Convolutional Neural Network (CNN) implemented from scratch with Keras and Tensorflow. Given the source luminance, it predicts the corresponding a and b color channels of the image in the CIE Lab colorspace.
							</p>
							<a class="image fit"><img src="images/approach.png" alt="" /></a>
						</section>
					</div>
					<div class="6u">
						<section>
							<h3>State of the art</h3>
							<p> </p>
						</section>	
						<section>
							<h3>Dataset</h3>
							<a class="image fit"><img src="BW_bird.png" alt="" /></a>
							<p> We used 2 datasets to train our model; ImageNet (ref) and a Flowers Image Dataset (ref). Predicting color has the nice property that training data is practically free. There is no need for humans to label data because any color photo can be used as a training example, simply by taking the image's L channel (luminance) as input and its ab channels (color) as the supervisory signal. Therefore, our algorithm uses a self-supervised approach to train by previously converting the source images to grayscale.</p>
						</section>
					</div>
				</div>
			</div>
		</section>			

		<section id="dataset" class="wrapper style1">
			<header class="major">
				<h2>Dataset generation</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="7u">
						<section>
							<h2>CIE Lab Color space</h2>
							<a class="image fit"><img src="images/Quantized_ab_color_space.png" alt=""/></a>
							<p> We work on the CIE Lab Color space. It expresses color as three numerical values, L* for the lightness and a* and b* for the green–red and blue–yellow color components. CIELAB was designed to be perceptually uniform with respect to human color vision, meaning that the same amount of numerical change in these values corresponds to about the same amount of visually perceived change.</p>
						</section>
					</div>
					<div class="5u">
						<section>
							<h2>Class rebalancing</h2>
							<p> Explain class rebalancing and class Probabilities to Point estimates</p>
						</section>
					</div>
				</div>
			</div>
		</section>

		<section id="architecture" class="wrapper style1">
			<header class="major">
				<h2>Architecture</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="7u">
						<section>
							<h2>First approach: BEGAN Architecture</h2>
							<a class="image fit"><img src="images/architecture.png" alt="" /></a>
							<p> The architecture proposed in the <a href="https://arxiv.org/pdf/1603.08511.pdf"> Colorful Image Colorization paper </a> consists in a CNN composed by 8 layers.
							Each layer refers to a block of 2 or 3 repeated convolution layers with a 3x3 filter and a ReLU activation function, followed by a Batch Normalization layer. To preserve the resolution within the block, a padding filter is used. <br> <br>

							At conv5 and conv6 a dilated convolution is applied to keep the output resolutions high and avoid the need of upsampling. A dilated convolution is a convolution with gaps (filled in with '0') in the filter. It is effective for a broader view of the input to capture more contextual information and it enables a faster run-time with less parameters. <br> <br>

							The following table lists the layers used in the architecture during training time.

							<a class="image fit"><img src="images/table_cnn.png" alt="" /></a>

							<b>X</b>: Spatial resolution of output <br> 
							<b>C</b>: Number of channels of output <br>
							<b>S</b>: Computation stride, values greater than 1 indicate downsampling following convolution, values less than 1 indicate upsampling preceding convolution <br>
							<b>D</b>: Kernel dilation <br>
							<b>Sa</b>: Accumulated stride across all preceding layers <br>
							<b>De</b>: Effective dilation of the layer with respect to the input (layer dilation times accumulated stride)<br>
							<b>BN</b>: Whether BatchNorm layer was used after layer
							<b>L</b>: Whether a 1x1 conv and cross-entropy loss layer was imposed.<br> <br>

							The model does not have any pool layer, all changes in size between conv blocks are achieved through spatial downsampling (by increasing stride) or upsampling.
							</p>
						</section>
					</div>
					<div class="5u">
						<section>
							<h2>Loss Function: Weighted cross-entropy</h2>
							<a class="image fit"><img src="images/loss_function.png" alt="" /></a>
							<p>The problem is treated as a multinomial classification. The ab output is quantified in Q=313 values. For a given input X of dims HxWx1, our architecture learns a mapping Zpred = G(X) to a probability distribution of the Q possible colors values. Therefore, Zpred has dimensions HxWxQ. <br>
							In order to compare Z predicted against the ground truth values of the image Y, we define a soft-encoding Z = inverse(H)(Y) that transforms the ground truth color to a 1-hot vector Z with the nearest 5 quantized bins ab using a gaussian kernel.<br>
							To account for the imbalance problem, a reweighting of the loss function based in the pixel's rarity is defined. Each pixels is weighted by a factor w of dims Qx1 based on its closest ab bin. The following formula shows how the reweighting matrix is obtained. </p>
						</section>
					</div>
				</div>

			</div>
		</section>

		<section id="experiments" class="wrapper style1">
			<header class="major">
				<h2>Experiments</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Overfitting a mini-batch</h2>
							<a class="image fit"><img src="images/mini_batch_overfitted.png" alt=""/></a>
							<p> Explain</p>
						</section>
					</div>
					<div class="6u">
						<section>
							<h2>Trainig with different loss functions</h2>
							
							<img src="images/mse_mini_batch.png" alt="" style="width:512px;"> 
							<img src="images/categorical_cross_entropy_minibatch.png" alt="" style="width:512px;">
							
							<p> Explain </p>
						</section>
					</div>
				</div>	
			</div>					
		</section>

		<section id="problems_solutions" class="wrapper style1">
			<header class="major">
				<h2>Problems and solutions</h2>
			</header>
			<div class="container">
				<div class="row">
					<div class="4u">
						<section>
							<h2>Loss turns into 'nan'</h2>
							<p> Explain</p>
						</section>
					</div>
					<div class="4u">
						<section>
							<h2>The model only produces red images</h2>
							<a class="image fit"><img src="images/problem_red_outputs.png" alt="" /></a>
							<p> Explain </p>
						</section>
					</div>
					<div class="4u">
						<section>
							<h2>First results</h2>
							<a class="image fit"><img src="images/first_results.png" alt="" /></a>
							<p> Explain </p>
						</section>
					</div>
				</div>	
			</div>
		</section>
		
		<section id="Final approach" class="wrapper style1">
			<header class="major">
				<h2>Final approach</h2>
			</header>
			<div class="container">
				
			</div>
		</section>
		
		<section id="future_work" class="wrapper style1">
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							<h2>Future work</h2>
							<p>....</p>
						</section>
					</div>

				</div>
			</div>
		</section>
		
		<section id="references" class="wrapper style2">
			<div class="container">
				<div class="row">
					<div class="6u">
						<section>
							
						</section>
					</div>
				</div>
			</div>
		</section>

	</body>
</html>
